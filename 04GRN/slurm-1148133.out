2025-01-21 00:42:38,556 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.16.2.1:42511'
2025-01-21 00:42:38,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.16.2.1:43121'
2025-01-21 00:42:38,651 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.16.2.1:41091'
2025-01-21 00:42:38,657 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.16.2.1:45903'
2025-01-21 00:42:41,762 - distributed.worker - INFO -       Start worker at:     tcp://172.16.2.1:46701
2025-01-21 00:42:41,763 - distributed.worker - INFO -          Listening to:     tcp://172.16.2.1:46701
2025-01-21 00:42:41,763 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-0
2025-01-21 00:42:41,763 - distributed.worker - INFO -          dashboard at:           172.16.2.1:41346
2025-01-21 00:42:41,764 - distributed.worker - INFO - Waiting to connect to: tcp://159.237.145.53:34618
2025-01-21 00:42:41,764 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:41,764 - distributed.worker - INFO -               Threads:                          2
2025-01-21 00:42:41,765 - distributed.worker - INFO -                Memory:                   5.59 GiB
2025-01-21 00:42:41,765 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m4bq2x8b
2025-01-21 00:42:41,765 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:41,801 - distributed.worker - INFO -       Start worker at:     tcp://172.16.2.1:43119
2025-01-21 00:42:41,801 - distributed.worker - INFO -          Listening to:     tcp://172.16.2.1:43119
2025-01-21 00:42:41,802 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-3
2025-01-21 00:42:41,802 - distributed.worker - INFO -          dashboard at:           172.16.2.1:36838
2025-01-21 00:42:41,802 - distributed.worker - INFO - Waiting to connect to: tcp://159.237.145.53:34618
2025-01-21 00:42:41,803 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:41,803 - distributed.worker - INFO -               Threads:                          2
2025-01-21 00:42:41,804 - distributed.worker - INFO -                Memory:                   5.59 GiB
2025-01-21 00:42:41,804 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mj_91rsz
2025-01-21 00:42:41,805 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:41,811 - distributed.worker - INFO -       Start worker at:     tcp://172.16.2.1:34072
2025-01-21 00:42:41,812 - distributed.worker - INFO -          Listening to:     tcp://172.16.2.1:34072
2025-01-21 00:42:41,813 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-1
2025-01-21 00:42:41,813 - distributed.worker - INFO -          dashboard at:           172.16.2.1:45535
2025-01-21 00:42:41,814 - distributed.worker - INFO - Waiting to connect to: tcp://159.237.145.53:34618
2025-01-21 00:42:41,814 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:41,814 - distributed.worker - INFO -               Threads:                          2
2025-01-21 00:42:41,815 - distributed.worker - INFO -                Memory:                   5.59 GiB
2025-01-21 00:42:41,815 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-az2gj21f
2025-01-21 00:42:41,816 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:41,825 - distributed.worker - INFO -       Start worker at:     tcp://172.16.2.1:43482
2025-01-21 00:42:41,826 - distributed.worker - INFO -          Listening to:     tcp://172.16.2.1:43482
2025-01-21 00:42:41,826 - distributed.worker - INFO -           Worker name:           SLURMCluster-0-2
2025-01-21 00:42:41,827 - distributed.worker - INFO -          dashboard at:           172.16.2.1:38004
2025-01-21 00:42:41,827 - distributed.worker - INFO - Waiting to connect to: tcp://159.237.145.53:34618
2025-01-21 00:42:41,827 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:41,828 - distributed.worker - INFO -               Threads:                          2
2025-01-21 00:42:41,828 - distributed.worker - INFO -                Memory:                   5.59 GiB
2025-01-21 00:42:41,829 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vqd8bvvn
2025-01-21 00:42:41,829 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:43,755 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-01-21 00:42:43,755 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-01-21 00:42:43,756 - distributed.worker - INFO -         Registered to: tcp://159.237.145.53:34618
2025-01-21 00:42:43,756 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:43,757 - distributed.core - INFO - Starting established connection to tcp://159.237.145.53:34618
2025-01-21 00:42:43,757 - distributed.worker - INFO -         Registered to: tcp://159.237.145.53:34618
2025-01-21 00:42:43,757 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-01-21 00:42:43,757 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:43,758 - distributed.core - INFO - Starting established connection to tcp://159.237.145.53:34618
2025-01-21 00:42:43,758 - distributed.worker - INFO -         Registered to: tcp://159.237.145.53:34618
2025-01-21 00:42:43,759 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-01-21 00:42:43,759 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:43,760 - distributed.core - INFO - Starting established connection to tcp://159.237.145.53:34618
2025-01-21 00:42:43,760 - distributed.worker - INFO -         Registered to: tcp://159.237.145.53:34618
2025-01-21 00:42:43,760 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:43,761 - distributed.core - INFO - Starting established connection to tcp://159.237.145.53:34618
2025-01-21 00:47:03,348 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-01-21 00:47:03,349 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-01-21 00:47:03,389 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-01-21 00:47:03,413 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-01-21 00:49:23,867 - distributed.worker - INFO - Stopping worker at tcp://172.16.2.1:46701. Reason: scheduler-close
2025-01-21 00:49:23,867 - distributed.worker - INFO - Stopping worker at tcp://172.16.2.1:43482. Reason: scheduler-close
2025-01-21 00:49:23,867 - distributed.worker - INFO - Stopping worker at tcp://172.16.2.1:34072. Reason: scheduler-close
2025-01-21 00:49:23,868 - distributed.worker - INFO - Stopping worker at tcp://172.16.2.1:43119. Reason: scheduler-close
2025-01-21 00:49:23,873 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.16.2.1:43774 remote=tcp://159.237.145.53:34618>
Traceback (most recent call last):
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/tornado/gen.py", line 766, in run
    value = future.result()
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://172.16.2.1:43774 remote=tcp://159.237.145.53:34618>: Stream is closed
2025-01-21 00:49:23,875 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.16.2.1:43770 remote=tcp://159.237.145.53:34618>
Traceback (most recent call last):
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/tornado/gen.py", line 766, in run
    value = future.result()
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://172.16.2.1:43770 remote=tcp://159.237.145.53:34618>: Stream is closed
2025-01-21 00:49:23,875 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.16.2.1:43776 remote=tcp://159.237.145.53:34618>
Traceback (most recent call last):
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/tornado/gen.py", line 766, in run
    value = future.result()
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://172.16.2.1:43776 remote=tcp://159.237.145.53:34618>: Stream is closed
2025-01-21 00:49:23,875 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.16.2.1:43772 remote=tcp://159.237.145.53:34618>
Traceback (most recent call last):
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/tornado/gen.py", line 766, in run
    value = future.result()
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://172.16.2.1:43772 remote=tcp://159.237.145.53:34618>: Stream is closed
2025-01-21 00:49:23,936 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.16.2.1:43121'. Reason: scheduler-close
2025-01-21 00:49:23,938 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.16.2.1:42511'. Reason: scheduler-close
2025-01-21 00:49:23,939 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.16.2.1:41091'. Reason: scheduler-close
2025-01-21 00:49:23,940 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('infer_partial_network-49ab41becc918757a1e5a2375148d6c8')" coro=<Worker.execute() done, defined at /home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
2025-01-21 00:49:23,940 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('infer_partial_network-612a6b10f434158ef7a111668c49cfce')" coro=<Worker.execute() done, defined at /home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
2025-01-21 00:49:23,940 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('infer_partial_network-c749d3b50e7b55ba247fdf97163325b5')" coro=<Worker.execute() done, defined at /home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
2025-01-21 00:49:23,941 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('infer_partial_network-6ff84a5a8f7e637736137055b588a615')" coro=<Worker.execute() done, defined at /home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
2025-01-21 00:49:23,942 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('infer_partial_network-2e585de98cb8154a30d590bbdf1252e7')" coro=<Worker.execute() done, defined at /home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
2025-01-21 00:49:23,943 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('infer_partial_network-b146ef75e75aa4380f1d19cd223a2d3f')" coro=<Worker.execute() done, defined at /home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
2025-01-21 00:49:23,945 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.16.2.1:45903'. Reason: scheduler-close
2025-01-21 00:49:23,949 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('infer_partial_network-9b2360c8da9cacd486dd01a5e182aecf')" coro=<Worker.execute() done, defined at /home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
2025-01-21 00:49:23,950 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('infer_partial_network-23063334a7ee8ae04aac8623b893e9ae')" coro=<Worker.execute() done, defined at /home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
slurmstepd: error: *** JOB 1148133 ON nodo01 CANCELLED AT 2025-01-21T00:49:23 ***
2025-01-21 00:49:23,977 - distributed._signals - INFO - Received signal SIGTERM (15)
2025-01-21 00:49:23,978 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.16.2.1:42511'. Reason: signal-15
2025-01-21 00:49:23,979 - distributed.nanny - INFO - Nanny asking worker to close. Reason: signal-15
2025-01-21 00:49:23,979 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.16.2.1:43121'. Reason: signal-15
2025-01-21 00:49:23,980 - distributed.nanny - INFO - Nanny asking worker to close. Reason: signal-15
2025-01-21 00:49:23,981 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.16.2.1:41091'. Reason: signal-15
2025-01-21 00:49:23,981 - distributed.nanny - INFO - Nanny asking worker to close. Reason: signal-15
2025-01-21 00:49:23,981 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.16.2.1:45903'. Reason: signal-15
2025-01-21 00:49:23,982 - distributed.nanny - INFO - Nanny asking worker to close. Reason: signal-15
2025-01-21 00:49:24,004 - distributed.nanny - INFO - Worker process 108839 was killed by signal 15
2025-01-21 00:49:24,008 - distributed.nanny - INFO - Worker process 108847 was killed by signal 15
2025-01-21 00:49:24,013 - distributed.nanny - INFO - Worker process 108843 was killed by signal 15
2025-01-21 00:49:24,255 - distributed.nanny - INFO - Worker process 108835 was killed by signal 15
2025-01-21 00:49:24,257 - distributed.dask_worker - INFO - End worker
