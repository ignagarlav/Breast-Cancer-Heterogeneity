2025-01-21 00:42:39,784 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.16.2.1:34091'
2025-01-21 00:42:39,823 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.16.2.1:46432'
2025-01-21 00:42:39,824 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.16.2.1:34539'
2025-01-21 00:42:39,825 - distributed.nanny - INFO -         Start Nanny at: 'tcp://172.16.2.1:42376'
2025-01-21 00:42:42,872 - distributed.worker - INFO -       Start worker at:     tcp://172.16.2.1:46488
2025-01-21 00:42:42,872 - distributed.worker - INFO -          Listening to:     tcp://172.16.2.1:46488
2025-01-21 00:42:42,873 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-2
2025-01-21 00:42:42,873 - distributed.worker - INFO -          dashboard at:           172.16.2.1:38807
2025-01-21 00:42:42,874 - distributed.worker - INFO - Waiting to connect to: tcp://159.237.145.53:34618
2025-01-21 00:42:42,874 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:42,875 - distributed.worker - INFO -               Threads:                          2
2025-01-21 00:42:42,875 - distributed.worker - INFO -                Memory:                   5.59 GiB
2025-01-21 00:42:42,879 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hz6t4gf7
2025-01-21 00:42:42,880 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:43,376 - distributed.worker - INFO -       Start worker at:     tcp://172.16.2.1:44041
2025-01-21 00:42:43,377 - distributed.worker - INFO -          Listening to:     tcp://172.16.2.1:44041
2025-01-21 00:42:43,377 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-0
2025-01-21 00:42:43,378 - distributed.worker - INFO -          dashboard at:           172.16.2.1:34603
2025-01-21 00:42:43,378 - distributed.worker - INFO - Waiting to connect to: tcp://159.237.145.53:34618
2025-01-21 00:42:43,378 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:43,379 - distributed.worker - INFO -               Threads:                          2
2025-01-21 00:42:43,379 - distributed.worker - INFO -                Memory:                   5.59 GiB
2025-01-21 00:42:43,379 - distributed.worker - INFO -       Start worker at:     tcp://172.16.2.1:34735
2025-01-21 00:42:43,379 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2gl8hnd9
2025-01-21 00:42:43,380 - distributed.worker - INFO -          Listening to:     tcp://172.16.2.1:34735
2025-01-21 00:42:43,380 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:43,380 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-1
2025-01-21 00:42:43,380 - distributed.worker - INFO -       Start worker at:     tcp://172.16.2.1:43551
2025-01-21 00:42:43,381 - distributed.worker - INFO -          dashboard at:           172.16.2.1:42176
2025-01-21 00:42:43,381 - distributed.worker - INFO -          Listening to:     tcp://172.16.2.1:43551
2025-01-21 00:42:43,381 - distributed.worker - INFO - Waiting to connect to: tcp://159.237.145.53:34618
2025-01-21 00:42:43,381 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-3
2025-01-21 00:42:43,382 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:43,382 - distributed.worker - INFO -          dashboard at:           172.16.2.1:33491
2025-01-21 00:42:43,382 - distributed.worker - INFO -               Threads:                          2
2025-01-21 00:42:43,382 - distributed.worker - INFO - Waiting to connect to: tcp://159.237.145.53:34618
2025-01-21 00:42:43,383 - distributed.worker - INFO -                Memory:                   5.59 GiB
2025-01-21 00:42:43,383 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:43,383 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hksl9d8s
2025-01-21 00:42:43,384 - distributed.worker - INFO -               Threads:                          2
2025-01-21 00:42:43,384 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:43,384 - distributed.worker - INFO -                Memory:                   5.59 GiB
2025-01-21 00:42:43,385 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zn8vt7os
2025-01-21 00:42:43,385 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:44,181 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-01-21 00:42:44,181 - distributed.worker - INFO -         Registered to: tcp://159.237.145.53:34618
2025-01-21 00:42:44,182 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:44,183 - distributed.core - INFO - Starting established connection to tcp://159.237.145.53:34618
2025-01-21 00:42:44,578 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-01-21 00:42:44,579 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-01-21 00:42:44,579 - distributed.worker - INFO -         Registered to: tcp://159.237.145.53:34618
2025-01-21 00:42:44,580 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:44,580 - distributed.worker - INFO -         Registered to: tcp://159.237.145.53:34618
2025-01-21 00:42:44,581 - distributed.core - INFO - Starting established connection to tcp://159.237.145.53:34618
2025-01-21 00:42:44,581 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:44,581 - distributed.core - INFO - Starting established connection to tcp://159.237.145.53:34618
2025-01-21 00:42:44,654 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-01-21 00:42:44,655 - distributed.worker - INFO -         Registered to: tcp://159.237.145.53:34618
2025-01-21 00:42:44,656 - distributed.worker - INFO - -------------------------------------------------
2025-01-21 00:42:44,657 - distributed.core - INFO - Starting established connection to tcp://159.237.145.53:34618
2025-01-21 00:47:03,362 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-01-21 00:47:03,364 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-01-21 00:47:03,365 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-01-21 00:47:03,369 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-01-21 00:49:23,867 - distributed.worker - INFO - Stopping worker at tcp://172.16.2.1:34735. Reason: scheduler-close
2025-01-21 00:49:23,867 - distributed.worker - INFO - Stopping worker at tcp://172.16.2.1:43551. Reason: scheduler-close
2025-01-21 00:49:23,867 - distributed.worker - INFO - Stopping worker at tcp://172.16.2.1:46488. Reason: scheduler-close
2025-01-21 00:49:23,870 - distributed.worker - INFO - Stopping worker at tcp://172.16.2.1:44041. Reason: scheduler-close
2025-01-21 00:49:23,876 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.16.2.1:43780 remote=tcp://159.237.145.53:34618>
Traceback (most recent call last):
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/tornado/gen.py", line 766, in run
    value = future.result()
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://172.16.2.1:43780 remote=tcp://159.237.145.53:34618>: Stream is closed
2025-01-21 00:49:23,874 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.16.2.1:43782 remote=tcp://159.237.145.53:34618>
Traceback (most recent call last):
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/tornado/gen.py", line 766, in run
    value = future.result()
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://172.16.2.1:43782 remote=tcp://159.237.145.53:34618>: Stream is closed
2025-01-21 00:49:23,872 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.16.2.1:43778 remote=tcp://159.237.145.53:34618>
Traceback (most recent call last):
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/tornado/gen.py", line 766, in run
    value = future.result()
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://172.16.2.1:43778 remote=tcp://159.237.145.53:34618>: Stream is closed
2025-01-21 00:49:23,871 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.16.2.1:43784 remote=tcp://159.237.145.53:34618>
Traceback (most recent call last):
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/tornado/gen.py", line 766, in run
    value = future.result()
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://172.16.2.1:43784 remote=tcp://159.237.145.53:34618>: Stream is closed
2025-01-21 00:49:23,936 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.16.2.1:34091'. Reason: scheduler-close
2025-01-21 00:49:23,938 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.16.2.1:46432'. Reason: scheduler-close
2025-01-21 00:49:23,938 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('infer_partial_network-af1bc0bbd9655eab1759a83a3c9648a4')" coro=<Worker.execute() done, defined at /home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
2025-01-21 00:49:23,939 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('infer_partial_network-c281b56c0798ad5d7caab1bd84701032')" coro=<Worker.execute() done, defined at /home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
2025-01-21 00:49:23,940 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.16.2.1:42376'. Reason: scheduler-close
2025-01-21 00:49:23,941 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('infer_partial_network-5ff8923f45d4821408d7518f69a7ed99')" coro=<Worker.execute() done, defined at /home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
2025-01-21 00:49:23,941 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('infer_partial_network-cf2080cd4349ce6b589f6ad0ad100104')" coro=<Worker.execute() done, defined at /home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
2025-01-21 00:49:23,943 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://172.16.2.1:34539'. Reason: scheduler-close
2025-01-21 00:49:23,943 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('infer_partial_network-497e45c285d8c1667ba7852d94b128de')" coro=<Worker.execute() done, defined at /home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
2025-01-21 00:49:23,945 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('infer_partial_network-92a640f7917f89cf74bec868a5533f8f')" coro=<Worker.execute() done, defined at /home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
2025-01-21 00:49:23,945 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('infer_partial_network-d7a188ab7c9492109d013d11e0cd93c2')" coro=<Worker.execute() done, defined at /home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
2025-01-21 00:49:23,947 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute('infer_partial_network-c9b3ed34cd4a586fb55f1eadffd144a0')" coro=<Worker.execute() done, defined at /home/igarzonalva/.conda/envs/arboreto-env/lib/python3.10/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
slurmstepd: error: *** JOB 1148134 ON nodo01 CANCELLED AT 2025-01-21T00:49:23 ***
2025-01-21 00:49:23,983 - distributed._signals - INFO - Received signal SIGTERM (15)
2025-01-21 00:49:23,983 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.16.2.1:34091'. Reason: signal-15
2025-01-21 00:49:23,984 - distributed.nanny - INFO - Nanny asking worker to close. Reason: signal-15
2025-01-21 00:49:23,984 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.16.2.1:46432'. Reason: signal-15
2025-01-21 00:49:23,985 - distributed.nanny - INFO - Nanny asking worker to close. Reason: signal-15
2025-01-21 00:49:23,985 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.16.2.1:34539'. Reason: signal-15
2025-01-21 00:49:23,986 - distributed.nanny - INFO - Nanny asking worker to close. Reason: signal-15
2025-01-21 00:49:23,986 - distributed.nanny - INFO - Closing Nanny at 'tcp://172.16.2.1:42376'. Reason: signal-15
2025-01-21 00:49:23,987 - distributed.nanny - INFO - Nanny asking worker to close. Reason: signal-15
2025-01-21 00:49:24,008 - distributed.nanny - INFO - Worker process 108865 was killed by signal 15
2025-01-21 00:49:24,009 - distributed.nanny - INFO - Worker process 108861 was killed by signal 15
2025-01-21 00:49:24,010 - distributed.nanny - INFO - Worker process 108858 was killed by signal 15
2025-01-21 00:49:24,011 - distributed.nanny - INFO - Worker process 108853 was killed by signal 15
2025-01-21 00:49:24,014 - distributed.dask_worker - INFO - End worker
